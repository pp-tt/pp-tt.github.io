<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TensorFlow on 噗通 🍀</title>
    <link>https://pp-tt.github.io.git/notes/tensorflow/</link>
    <description>Recent content in TensorFlow on 噗通 🍀</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://pp-tt.github.io.git/notes/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
      <description>TensorFlow 笔记（一） # 介绍TensorFlow基本API和神经网络训练的基本流程：准备数据（数据加载，数据乱序，数据分割，数据配对），搭建神经网络，参数优化，计算LOSS，计算ACC，画出LOSS和ACC图。通过鸢尾花分类串起来以上操作。
1.神经网络的计算过程 # 反向传播计算示例代码：
import tensorflow as tf w = tf.Variable(tf.constant(5, dtype=tf.float32)) # 定义可变变量 lr = 0.999 # 定义学习率 epoch = 40 # 迭代次数 for epoch in range(epoch): with tf.GradientTape() as tape: # with 结构起到grads框起了梯度的计算过程 loss = tf.square(w + 1) grads = tape.gradient(loss, w) # .gradient函数告知谁对谁求导 w.assign_sub(lr * grads) # .assign_sub 对变量做自减，即 w -= lr * grads print(f&amp;#34;After {epoch} epoch, w is {w.numpy()}, loss is {loss}&amp;#34;) 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%BA%8C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%BA%8C/</guid>
      <description>TensorFlow 笔记（二） # 介绍神经网络的优化过程，主要有：
神经网络复杂度 指数衰减学习率 激活函数 损失函数 欠拟合与过拟合 正则化减少过拟合 优化器更新网络参数 1. 预备知识 # tf.where(条件语句，真返回A，假返回B)：条件语句，真返回A，假返回B a = tf.constant([1, 2, 3, 1, 1]) b = tf.constant([0, 1, 3, 4, 5]) c = tf.where(tf.greater(a, b), a, b) # 若a&amp;gt;b，返回 a 对应位置的元素，否则返回 b 对应位置的元素 输出：c = tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) np.random.RandomState.rand(维度)：返回 [0, 1) 之间的随机数 rdm = np.random.RandomState(seed=1) a = rdm.rand() # 返回一个随机标量 b = rdm.rand(2, 3) # 返回维度为2行3列随机数矩阵 np.vstack(数组1， 数组2)：将两个数组按垂直方向叠加 a = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%B8%89/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%B8%89/</guid>
      <description>TensorFlow 笔记（三） # 利用 TensorFlow API 搭建神经网络，利用六步法，采用 20 行左右代码重写 iris 分类。
1. 六步法 # import train，test model = tf.keras.models.Sequential model.compile model.fit model.summary 2. 相关 API 解析 # 2.1 model # # 描述各层网络 model = tf.keras.models.Sequential([ 网络结构 ]) 网络结构举例：
拉直层： tf.keras.layers.Flatten() 全连接层 tf.keras.layers.Dense(神经元个数, activation=&amp;#39;激活函数&amp;#39;, kernel_regularizer=哪种正则化) activation (字符串给出) 可选：relu, softmax, sigmoid, tanh kernel_regularizer 可选：tf.keras.regularizers.l1(), tf.keras.regularizers.l2() 卷积层 tf.keras.layers.Conv2D(filters = 卷积个数, kernel_size = 卷积, strides = 卷积步长, padding = &amp;#34;vaild&amp;#34; or &amp;#34;same&amp;#34;) LSTM层 tf.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E5%9B%9B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E5%9B%9B/</guid>
      <description>TensorFlow 笔记（四） # 对六步法进行扩展，扩展内容如下：
自制数据集，解决本领域应用 数据增强，扩充数据集 断点续训，存取模型 参数提取，把参数存入文本 acc / loss 可视化，查看训练效果 应用程序，给图识物 1. 自制数据集 # &amp;#34;&amp;#34;&amp;#34; import &amp;#34;&amp;#34;&amp;#34; from matplotlib.pyplot import cla from sklearn import metrics from tensorflow.keras.layers import Dense, Flatten import tensorflow as tf from tensorflow.keras import Model from PIL import Image import numpy as np &amp;#34;&amp;#34;&amp;#34; train, test &amp;#34;&amp;#34;&amp;#34; train_img_path = &amp;#39;TensorFlow 笔记/mnist_image_label/mnist_train_jpg_60000&amp;#39; test_img_path = &amp;#39;TensorFlow 笔记/mnist_image_label/mnist_test_jpg_10000&amp;#39; train_label = &amp;#39;TensorFlow 笔记/mnist_image_label/mnist_train_jpg_60000.txt&amp;#39; test_label = &amp;#39;TensorFlow 笔记/mnist_image_label/mnist_test_jpg_10000.txt&amp;#39; def generateds(path, txt): f = open(txt) contents = f.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%BA%94/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pp-tt.github.io.git/notes/tensorflow/tensorflow-%E7%AC%94%E8%AE%B0%E4%BA%94/</guid>
      <description>TensorFlow 笔记（五） # 介绍卷积神经网络的一些概念，并利用卷积神经网络对 Cifar10 数据集进行分类。
1.全连接NN # 每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果。
实际应用时会先对原始图像进行特征提取，再把提取到的特征送给全连接网络。
2.卷积 # 卷积计算是一种有效的提取图像特征的方法。
一般会用一个正方形的卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘，求和再加上偏执项得到输出特征的一个像素点。
输入特征图的深度（channel数）决定了当前层卷积核的深度；当前层卷积核的个数，决定了当前层输出特征图的深度。
tf.keras.layers.Conv2D( filters = ,	# 卷积核个数 kernel_size = ,	# 卷积核尺寸 # stride = ,	# 滑动步长 padding = ,	# &amp;#34;same&amp;#34; or &amp;#34;valid&amp;#34; activation = ,	# &amp;#34;relu&amp;#34;，&amp;#34;sofmax&amp;#34;，&amp;#34;tanh&amp;#34;，&amp;#34;sigmoid&amp;#34; # input_shape = ,	# (高，宽，通道数) ) 3. 感受野 # 卷积神经网络各输出特征图中的每个像素点在原始输入图片上映射区域的大小。 \[padding=\begin{cases}SAME（全零填充）, &amp;amp; \frac {入长}{步长}（向上取整） \\ VALID（不全零填充）, &amp;amp; \frac {入长-核长&amp;#43;1}{步长}（向上取整） \end{cases}\] 4. 批标准化（Batch Normalization，BN） # 标准化：使数据符合均值为 0，1 位标准差的分布</description>
    </item>
    
  </channel>
</rss>
